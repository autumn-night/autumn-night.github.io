---   
layout: post   
title:  "机器学习学习笔记"   
date:   2025-1-14 10:18:54 +0800   
categories: jekyll update   
---   
#### 1. 前言   
需要提前说明的是：本人数学不好，并且并不关心机器学习的本质，只是将其视为一种处理数字的工具。   
   
因此在学习过程中，我并没有试图去理解每个模型背后的数学原理，仅仅了解了模型的结构与模型的作用――会写会用达到了我的目的。而且由于一些原因，本人所学的模型并不是很多，所以文章并不会很长。   
   
所以本文的主要内容是关于模型的作用的，会涉及部分模型的结构，基本不会涉及任何数学原理，仅仅是用来保证本人不会遗忘每个模型的特点的一篇笔记。   

内容上会分为非深度学习和深度学习两部分，写的顺序与结构会比较随心所欲，属于想起来什么写什么的那种。   
   
#### 2.1 非深度学习   
- 线性模型   
包括线性回归与逻辑回归两部分。线性回归用于回归任务，拟合一条直线；逻辑回归主要用于分类任务   
   
- 支持向量机(SVM)   
基础的SVM核心思路在于找到一个超平面，使得两种类别之间的间隔达到最大（间隔即两个类别中里超平面最近的点的距离）   
显然的，上述的SVM只能够支持对三维空间中的数据进行操作，并且假设了两种类别的关系为线性关系（三维空间的平面方程中只有两个变量）   
因此其产生了一种名为“核函数”的扩展，核函数可以将数据从低维空间映射到高位空间，从而实现了对非线性关系的分类操作。   
   
- k最近邻(KNN)
非常直白的一种分类算法，直接计算每个点到其他点的距离，将和该点距离最近的k个点中出现次数最多的类别当做该点的类别。   
   
- PCA与LDA
两种用来数据降维的机器学习算法，最大的不同之处在于PCA是无监督学习，LDA是监督学习。具体使用那种还需要根据实际情况来定（目前没用过）   
   
其他的模型要么是没学，要么是忘得差不多了，等回头用到了再补充吧
   
#### 2.2 深度学习   
   
有些深度学习方法存在很多细分，需要着重注意他们的隐藏层构建方式；不同的深度学习方法的模型构建差异还是很大的   
   
- 卷积神经网络(CNN)
层数不固定，维度也有一维和二维的。卷积操作主要通过卷积核实现，在卷积操作后经常会增加池化操作。不过随着模型层数的增加，为了减弱由于深度增加造成的退化，池化层的数量往往会减少。在卷积层、池化层之后和输出前往往会增加三个左右的全连接层   
主要用于二维图的特征提取（计算机视觉），也有将一维数据转化为二维灰度图进行特征提取的，能和特征提取有关系就可以考虑考虑CNN。也可以用于自然语言处理(NLP)任务，不过应用范围肯定不如循环神经网络(RNN)。
   
- 循环神经网络(RNN)   
简单的RNN一般是一组链式连接的函数，在进行长距离的学习时，会产生梯度爆炸或者梯度消失的问题。因此提出了LSTM等方法来解决此问题。   
主要用于NLP问题，也有用于计算机视觉部分的。不过NLP问题在本人写文章的时候，如果算力足够的话还是用transformer更多一点。   

- 生成对抗网络(GAN)
一般包含一个生成器和一个判别器，目的是使得生成器能够生成更加逼真的样本，或者提升判别器的泛用性。在训练的时候一般先固定生成器的参数然后训练判别器，之后在固定判别器的参数训练生成器，以此往复。不难看出来这是一种无监督学习方式   
生成器和判别器两个部分都可以单独拿出来使用，因此用途还算挺广。判别器可以用于分类问题；生成器可以用与图像生成、数据增强等多种问题。   
   
- 图神经网络(GNN)   
存在多种变体，如图卷积网络GCN、图注意力网络GAT、图循环网络GRN等。GCN主要通过聚合邻居信息后再对此信息进行处理，生成新的节点表示；GAT就是在GCN中引入了注意力机制。说的比较笼统，因为展开说会很复杂，而且我也不会展开说。   
能和图搭上边的都可以用图神经网络提取一下信息试试，但是由于实现的问题，往往图神经网络跑起来挺慢的。能够用于NLP问题、计算机视觉等领域（这俩咋啥模型都能用上），然后像化学分子、社交网络、思维导图等和图有关的自然可以使用图神经网络处理。   

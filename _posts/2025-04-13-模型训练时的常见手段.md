---   
layout: post   
title: "模型训练的常见手段"   
date: 2025-4-13 21:00:00 +0800   
categories: jekyll update   
---   
#### 1. 前言    
最近一直在做二分类模型的复现和调参，由于数据集本身的限制，一直面临着数据严重不平衡的问题。通过AI与网络上的资料学会了一些常见的处理方法，在此简单的做一个记录，希望能够为将来的自己或者是看到笔记的其他人提供一定的参考。   
   
#### 2. 大数据集模型的训练
在复现一篇使用GNN的模型时，遇到了一个问题：内存大小不足以支撑我将所有数据全部加载到内存中。   
这种情况的解决方法和我们往常的处理方式――将数据全部读取出来之后放入DataLoader中进行minibatch训练――大同小异。   
本人采用的是sqlite3管理本地数据库，因此只用在写传入Dataset类的时候，将其中的__getitem__()函数稍作更改即可。具体的实现方式如下所示：   
```python
from torch.utils.data import Dataset
import sqlite3
class Filedataset(Dataset):
    def __init__(self, pth):
        # 连接到数据库
        self.connection = sqlite3.connect(pth)
        self.cursor = self.connection.cursor()
        # 获取数据总数
        self.cursor.execute("SELECT COUNT(*) FROM data")
        self.length = self.cursor.fetchone()[0]
    def __len__(self):
        return self.length
    def __getitem__(self, idx):
        # 获取单条数据，此处默认数据存在data表中
        self.cursor.execute(f"SELECT * FROM data LIMIT 1 OFFSET {idx}")
        item = self.cursor.fetchone()
        # 根据你存入数据库中数据的形状，读取你的数据，这里只是一个简单的例子
        data = item[0]
        label = item[1]
        return data, label
    def __del__(self):
        # 确保在对象销毁时关闭数据库连接
        self.connection.close()
```   
需要指出的是，这份代码的执行效率很低，因为涉及到大量的IO操作。可能仅适合于单条数据过大，而整体数据量一般的数据进行训练。   
对于该问题或许存在着其他更优的解决方案，需要更多的学习。    
   
#### 3. 二分类不平衡数据集的训练   
不平衡数据集还是比较常见的，不过目前我遇到的数据集不平衡都很严重，因此从网络上找了很多相关的解决方案，在此特地记录一下。   
需要提醒的是，这一部分并没有数据增强方面的解决办法――我复现的模型很少能够使用数据增强，因此并没有对此方面进行关注   
##### 3.1 损失函数的选择   
+ 二分类常见的损失函数就是nn.BCEWithLogitsLoss函数，这个函数支持通过加权处理类别不平衡问题。顺带一提，多分类的nn.Crossentropyloss函数也支持这么搞
传入参数weight=your_weight即可，其中your_weight=负样本与正样本的比例，注意参数类型为torch.tensor。
+ 除了上述简单粗暴的加权方式之外，还有一种我用着效果比较明显的损失函数――Focalloss。该函数的实现思路不细讲了，网络上资料还挺多的，这里只贴一下实现方法：
```python
import torch.nn.functional as F
class BCEFocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None, reduction='mean'):
        """
        BCEWithLogitsLoss与Focal Loss的结合
        参数:
        - alpha: 正样本权重系数 (0 < alpha < 1)
        - gamma: 聚焦参数，控制易分类样本的权重衰减速率
        - pos_weight: 正样本的权重，可用于进一步平衡类别
        - reduction: 'mean'或'sum'或'none'
        """
        super(BCEFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight
        self.reduction = reduction
        
    def forward(self, logits, target):
        # BCE损失 (使用logits形式)
        if self.pos_weight is not None:
            bce_loss = F.binary_cross_entropy_with_logits(
                logits, target, pos_weight=self.pos_weight, reduction='none'
            )
        else:
            bce_loss = F.binary_cross_entropy_with_logits(
                logits, target, reduction='none'
            )
        # 计算预测概率
        probs = torch.sigmoid(logits)
        # 计算pt (正样本概率或负样本概率)
        pt = torch.where(target == 1, probs, 1 - probs)
        # 计算Focal Loss权重
        focal_weight = (1 - pt) ** self.gamma
        # 加入alpha权重
        if self.alpha is not None:
            alpha_weight = torch.where(target == 1, self.alpha, 1 - self.alpha)
            focal_weight = alpha_weight * focal_weight
        # 计算最终损失
        loss = focal_weight * bce_loss
        # 应用reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss
```   
值得注意的是，该损失函数使用的并不是传统的Focalloss中使用的Crossentropyloss函数，而是直接使用了BCEWithLogitsLoss函数作为损失函数。   
个人认为，这种实现方式可以更好的处理类别不平衡问题，因为Focalloss的设计初衷是用来处理难分类样本的。不过并没有经过实验验证是否会更好。   
##### 3.2 训练时候的注意事项
+ 如果采用minibatch训练的话，务必注意每个batch中是否一定存在一个少类别的样本，否则该batch的loss将被多类别的样本主导，导致loss极低，从而影响模型的收敛效果。
使用自定义的Sampler可以解决上述问题。Sampler可以指定向每个batch中放入多少个少类别样本，从而保证loss的稳定。下面给出一份具体的实现方式：   
```python
from torch.utils.data import Sampler
class BatchSampler(Sampler):
    """
    确保每个batch至少包含指定数量的正样本的采样器
    """
    def __init__(self, dataset, positive_indices, negative_indices, batch_size, pos_samples_per_batch=1):
        self.dataset = dataset
        self.positive_indices = positive_indices  # 所有正样本的索引列表
        self.negative_indices = negative_indices  # 所有负样本的索引列表
        self.batch_size = batch_size
        self.pos_samples_per_batch = min(pos_samples_per_batch, len(positive_indices))
        # 计算每个epoch的batch数量
        self.num_batches = len(negative_indices) // (batch_size - self.pos_samples_per_batch)
    def __iter__(self):
        # 对负样本进行洗牌
        neg_indices = np.random.permutation(self.negative_indices).tolist()
        # 循环使用正样本(因为正样本很少)
        pos_indices = self.positive_indices.copy()
        batches = []
        for i in range(self.num_batches):
            # 为当前batch选择正样本
            if len(pos_indices) < self.pos_samples_per_batch:
                # 如果正样本不够了，重新洗牌
                pos_indices = self.positive_indices.copy()
                np.random.shuffle(pos_indices)
            # 选择正样本
            batch_pos = pos_indices[:self.pos_samples_per_batch]
            pos_indices = pos_indices[self.pos_samples_per_batch:]
            # 选择负样本
            batch_neg = neg_indices[i * (self.batch_size - self.pos_samples_per_batch): 
                                  (i + 1) * (self.batch_size - self.pos_samples_per_batch)]
            # 组合并打乱当前batch的样本
            batch = batch_pos + batch_neg
            np.random.shuffle(batch)
            batches.extend(batch)
        return iter(batches)
    def __len__(self):
        return self.num_batches * self.batch_size
```   
可以注意到，上述Sampler带来了一个新的问题：在少类样本不够用时，从头开始重复使用样本，这样显然会导致模型对于训练集中少类样本的过拟合。   
针对这个新问题目前想到的解决办法是，引入一个测试集与早停操作监测模型是否产生了过拟合行为。不过这种方法在数据量有限的情况下不是十分的管用，可能有其他更优的解法。   